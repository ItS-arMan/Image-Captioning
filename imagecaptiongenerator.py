# -*- coding: utf-8 -*-
"""ImageCaptionGenerator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tRAoLZHU7wEEIDjknKhVA-GvUDlWGJLy
"""

import os
import numpy as np
import tensorflow as tf
from pycocotools.coco import COCO
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from tensorflow.keras.applications import InceptionV3
from tensorflow.keras.applications.inception_v3 import preprocess_input
from tensorflow.keras.preprocessing.image import load_img, img_to_array
import matplotlib.pyplot as plt

dataDir = '/content/coco'  # Set this to your dataset path
annFile = os.path.join(dataDir, 'annotations', 'captions_train2017.json')
imageDir = os.path.join(dataDir, 'train2017')

coco = COCO(annFile)

imgIds = coco.getImgIds()
captions = []
images = []

for imgId in imgIds[:5]:
    annIds = coco.getAnnIds(imgIds=imgId)
    anns = coco.loadAnns(annIds)
    for ann in anns:
        captions.append(ann['caption'])
        # Get the image file name and build its full path
        images.append(os.path.join(imageDir, coco.loadImgs(imgId)[0]["file_name"]))


def preprocess_captions(captions, num_words=5000, max_length=34):
    tokenizer = Tokenizer(num_words=num_words, oov_token="<unk>")
    tokenizer.fit_on_texts(captions)
    sequences = tokenizer.texts_to_sequences(captions)
    # Pad sequences to fixed max_length
    padded_captions = pad_sequences(sequences, padding='post', maxlen=max_length)
    word_index = tokenizer.word_index
    return tokenizer, padded_captions, word_index

max_length = 34
tokenizer, padded_captions, word_index = preprocess_captions(captions, max_length=max_length)
vocab_size = len(word_index) + 1

# For teacher forcing, we typically use input sequences that are one token shorter than the full caption.
# Define sequence_length as max_length - 1.
sequence_length = max_length - 1
image_paths = []

base_model = InceptionV3(weights="imagenet")
feature_extractor = tf.keras.Model(base_model.input, base_model.layers[-2].output)

def extract_features(image_paths):
    features = []
    for image_path in image_paths:
        try:
            img = load_img(image_path, target_size=(299, 299))
            img = img_to_array(img)
            img = np.expand_dims(img, axis=0)
            img = preprocess_input(img)
            features.append(feature_extractor.predict(img).reshape(-1))
        except Exception as e:
            print(f"Error loading image: {image_path}, error: {e}")
            continue
    return np.array(features)

image_features = extract_features(images)


X_image_train, X_image_val, y_caption_train, y_caption_val = train_test_split(
    image_features, padded_captions, test_size=0.2, random_state=42
)


def create_target_captions(captions, sequence_length):
    # For teacher forcing, use the caption without the last word as input
    # and the caption without the first word as the target output.
    captions_input = captions[:, :-1]
    captions_output = captions[:, 1:]
    # Pad to the fixed sequence_length (which is max_length-1)
    captions_input = pad_sequences(captions_input, padding='post', maxlen=sequence_length)
    captions_output = pad_sequences(captions_output, padding='post', maxlen=sequence_length)
    # One-hot encode the output captions
    captions_output = to_categorical(captions_output, num_classes=vocab_size)
    return captions_input, captions_output

y_caption_train_input, y_caption_train_output = create_target_captions(y_caption_train, sequence_length)
y_caption_val_input, y_caption_val_output = create_target_captions(y_caption_val, sequence_length)


embedding_dim = 256

from tensorflow.keras.layers import Input, Dense, Dropout, Embedding, LSTM, Add, RepeatVector, TimeDistributed
from tensorflow.keras.models import Model

# Image branch: input features of shape (2048,)
image_input = Input(shape=(2048,))
image_dense = Dense(embedding_dim, activation="relu")(image_input)
image_dropout = Dropout(0.5)(image_dense)
# Repeat image features to match the sequence length for every time step
image_repeated = RepeatVector(sequence_length)(image_dropout)

# Text branch: input is a sequence of tokens (length = sequence_length)
text_input = Input(shape=(sequence_length,))
text_embedding = Embedding(vocab_size, embedding_dim, mask_zero=True)(text_input)
# Set return_sequences=True to output a hidden state for each time step
text_lstm = LSTM(256, return_sequences=True)(text_embedding)


combined = Add()([image_repeated, text_lstm])

output = TimeDistributed(Dense(vocab_size, activation="softmax"))(combined)

# Build and compile the model
model = Model(inputs=[image_input, text_input], outputs=output)
model.compile(optimizer="adam", loss="categorical_crossentropy")


batch_size = 64
epochs = 5
history = model.fit(
    [X_image_train, y_caption_train_input],
    y_caption_train_output,
    validation_data=([X_image_val, y_caption_val_input], y_caption_val_output),
    batch_size=batch_size,
    epochs=epochs,
)


model.save("coco_image_caption_generator_model.h5")
import pickle
with open("coco_tokenizer.pkl", "wb") as f:
    pickle.dump(tokenizer, f)


plt.plot(history.history["loss"], label="Training Loss")
plt.plot(history.history["val_loss"], label="Validation Loss")
plt.legend()
plt.savefig("coco_training_history.png")
plt.show()